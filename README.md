### Project Overview: Kubernetes(kubeadm) Infrastructure Automation on AWS

**Disclaimer**: Application code by Author: Ryan Almeida
ğŸ“Œ GitHub Profile: Ryan Almeida[https://github.com/ryan-almeida]

**Goal**: Provision a scalable, containerized note-taking app on Kubernetes, deployed on EC2 instances using Terraform for infrastructure, Ansible for configuration management, and a CI/CD pipeline (Jenkins + Harness) for automation, using Terraform modules for networking and compute, and remote state in S3/DynamoDB.

## Kubernetes & Ansible:
One Control Node, One Master Node, and a Highly Available Worker Node setup across Two AZs.

**Components**:
- **Infrastructure (Terraform)**:
  - VPC, Subnets, Security Groups, Internet Gateway (IGW), Route Tables
  - EC2 instances for Kubernetes nodes
  - Auto Scaling Group (ASG) for worker nodes
  - Application Load Balancer (ALB)
  - S3 bucket and DynamoDB table for Terraform state

- **Configuration Management (Ansible)**:
  - Kubernetes installation and node setup
  - Application deployment configurations

- **Orchestration (Kubernetes)**:
  - Node.js app running as a containerized service
  - Ingress controller for routing

- **CI/CD (Jenkins + Harness)**:
  - Jenkins pipeline for building and testing
  - Harness for automated deployments

- **State**: 
  - S3 bucket and DynamoDB table for Terraform state.

---

### Project Structure
Following best practices:

```
./
â”œâ”€â”€ application-code/          # Node.js note-app source code
â”‚   â”œâ”€â”€ src/                  # App source files (e.g., server.js)
â”‚   â”œâ”€â”€ package.json          # Node.js dependencies and scripts
â”‚   â”œâ”€â”€ Dockerfile            # Docker image definition for the app
â”‚   â””â”€â”€ .dockerignore         # Ignore files for Docker build
â”œâ”€â”€ terraform/                # Terraform infra code
â”‚   â”œâ”€â”€ main.tf               # Root module: Calls networking and compute modules
â”‚   â”œâ”€â”€ variables.tf          # Root-level variables
â”‚   â”œâ”€â”€ outputs.tf            # Root-level outputs (e.g., cluster IPs)
â”‚   â”œâ”€â”€ provider.tf           # AWS provider config
â”‚   â”œâ”€â”€ versions.tf           # Terraform and provider versions, S3 backend
â”‚   â”œâ”€â”€ bootstrap/            # Bootstrap module for remote state management
â”‚   â”‚   â”œâ”€â”€ main.tf           # Manages S3 backend and DynamoDB for state locking
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ networking/       # VPC, subnets, IGW, etc.
â”‚       â”‚   â”œâ”€â”€ main.tf
â”‚       â”‚   â”œâ”€â”€ variables.tf
â”‚       â”‚   â””â”€â”€ outputs.tf
â”‚       â””â”€â”€ compute/          # EC2 instances for Kubernetes nodes
â”‚           â”œâ”€â”€ main.tf
â”‚           â”œâ”€â”€ variables.tf
â”‚           â””â”€â”€ outputs.tf
â”œâ”€â”€ ansible/                  # Ansible config for Kubernetes setup
â”‚   â”œâ”€â”€ inventory.yml         # List of EC2 instances (dynamically populated)
â”‚   â”œâ”€â”€ playbooks/
â”‚   â”‚   â”œâ”€â”€ install-k8s.yml   # Install Kubernetes (kubeadm, Docker, etc.)
â”‚   â”‚   â””â”€â”€ configure-cluster.yml  # Join nodes to the cluster
â”‚   â””â”€â”€ roles/                # Reusable Ansible roles (optional)
â”‚       â”œâ”€â”€ kubernetes/
â”‚       â”‚   â”œâ”€â”€ tasks/
â”‚       â”‚   â”œâ”€â”€ templates/
â”‚       â”‚   â””â”€â”€ vars/
â”œâ”€â”€ kubernetes/               # Kubernetes manifests for the note-app
â”‚   â”œâ”€â”€ deployment.yml        # Deployment for the Node.js app
â”‚   â”œâ”€â”€ service.yml           # Service to expose the app (e.g., LoadBalancer)
â”‚   â””â”€â”€ configmap.yml         # Optional: Config for the app (if needed)
â”œâ”€â”€ .gitignore                # Ignore transient files
â””â”€â”€ README.md                 # Project overview and setup instructions
```

#### Prerequisites

### Prerequisites
aws, aws cli, terraform, ansible, python installed and set up
3. Ansible installed (e.g via `pip3 install ansible --user`)

#### Deployment Guide

## A. Configuring Remote State Management with S3 and DynamoDB

**Run the Bootstrap**:
1. Navigate to the `bootstrap/` directory:
   ```bash
   cd bootstrap
   ```
2. Initialize and apply:
   ```bash
   terraform init
   terraform apply
   ```
   - Type `yes` when prompted to create the resources.
3. This creates the S3 bucket and DynamoDB table, storing the state locally in `bootstrap/terraform.tfstate`.

**Initialize the Remote Backend**:
1. Navigate to your main project directory

2. Run:
   ```bash
   terraform init
   ```
   - Terraform will detect the backend configuration and prompt you to migrate any existing local state to S3. Type `yes`.

### Why This Works
- The bootstrap step creates the S3 bucket and DynamoDB table independently.
- The main project then uses these resources to store its state remotely, with DynamoDB providing state locking to prevent concurrent modifications.


## B. Change the path in
- ansilbe_ssh_private_key_file
- ~/.ssh/config   - change the private ips of k8s nodes to your output from terraform. Get the private IPs from your Terraform outputs (e.g bash: terraform output worker_instance_ips in the terraform/ directory).

## C. Enable SSH from Control Node to Kubernetes Nodes

### Copy the Private Key to the Control Node:
Since the Kubernetes nodes uses `note-app-key.pub`, you need the matching private key (`note-app-key`) on the control node to SSH into them.

1. **Step 1**: Locate the Private Key on the machine where you ran `ssh-keygen` e.g VM1

2. **Step 2**: Copy the Private Key to the Control Node
From VM1, SCP the private key to the control node:

- Since `note-app-key` is the private key that matches control-nodeâ€™s public key (from the AWS key pair), use it directly in the scp command from VM1. Example:

```bash
scp -i /home/ec2-user/.ssh/note-app-key /home/ec2-user/.ssh/note-app-key ubuntu@<public-ip>:/home/ubuntu/.ssh/note-app-key
```

- **-i /home/ec2-user/.ssh/note-app-key**: Specifies the private key to authenticate with vm2.
- **Source**: /home/ec2-user/.ssh/note-app-key (the file to copy).
- **Destination**: ubuntu@1<public-ip>:/home/ubuntu/.ssh/note-app-key (where itâ€™s going on vm2).

### If It Fails:
Set File Permissions:
On vm1, ensure note-app-key has the right permissions:
```bash
chmod 600 /home/ec2-user/.ssh/note-app-key
```

`application-code/`
- Test locally: `docker build -t note-app . && docker run -p 3000:3000 note-app`.


`ansible/`
- Use Ansible roles for modularity (e.g., `kubernetes` role).
- Test playbook locally first with a single VM if possible.


`kubernetes/`
- Use `kubectl apply -f kubernetes/` to deploy manually first.


CI/CD (Jenkins + Harness)
- **CI (Jenkins)**:
  - **Pipeline**: `Jenkinsfile` to `npm test`, `docker build`, `docker push` to a registry (e.g., Docker Hub).
- **CD (Harness)**:

  - **Setup**: Install Jenkins on a separate EC2 or locally.
  - Start with manual `kubectl` deployment, then automate with Jenkins/Harness.

---
**Communication**:

The nodes have their communication routes via the `route tables`:
- **Master â†” Workers**: Both in private subnets, use 10.0.0.0/16 â†’ local to talk internally (e.g., API on 6443, pod traffic).
- **ALB â†’ Nodes**: ALB in public subnets (via IGW) sends traffic to nodes in private subnets (via Security Group rules, e.g., NodePort range).
- **Nodes â†’ Internet**: Not yetâ€”private route table lacks 0.0.0.0/0 â†’ nat-<id> (no NAT Gateway), so no outbound internet access (weâ€™ll add this later if needed for Docker pulls).

---
Terraform Taint:
`terraform taint module.compute.aws_autoscaling_group.workers` â†’ Marks ASG for recreation.
terraform apply â†’ Rebuilds ASG and instances.


Next Steps:

Ansible Setup

Automate the Process. Grok query - Step 2: Fetch Terraform Outputs
- Dynamically fetch the private IP addresses of your nodes from Terraform outputs and use them in your Ansible inventory.
- Add a step in your workflow (e.g., a shell script) to regenerate terraform_outputs.json after terraform apply.


Containerize App
k8s deployment
cicd

--- 
- Graceful shutdown.txt

- GPT query:

2ï¸âƒ£ Configure Ansible to Use the Control Node as a Bastion Host

---

### Workflow Steps
1. **Terraform**:
   - Provision VPC, subnets, 
   - Output IPs for Ansible.
2. **Ansible**:
   - Run `install-k8s.yml` to install Kubernetes.
   - Run `configure-cluster.yml` to set up the cluster.
3. **CI (Jenkins)**:
   - Build Docker image from `application-code/`.
   - Push to a registry.
4. **Kubernetes**:
   - Deploy the image manually with `kubectl` to test.
5. **CD (Harness)**:
   - Automate deployment to Kubernetes.

---